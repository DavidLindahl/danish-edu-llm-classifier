{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import krippendorff\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a91915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching Data\n",
    "\n",
    "df_mikkel = pd.read_json(\"annotations_mikkel.json\")\n",
    "df_david = pd.read_json(\"annotations_david.json\")\n",
    "df_carl = pd.read_json(\"annotations_carl.json\")\n",
    "\n",
    "#Fix of dataframe\n",
    "def parse_label_string(label_str):\n",
    "    return re.findall(r\"'(.*?)'\", label_str)\n",
    "\n",
    "df_mikkel['original_label'] = df_mikkel['original_label'].apply(parse_label_string)\n",
    "\n",
    "#Check:\n",
    "df_mikkel[\"original_label\"].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data and setting variables \n",
    "\n",
    "num_encoder = {'None': 0,\n",
    "               'Minimal': 1,\n",
    "               'Basic': 2,\n",
    "               'Good': 3,\n",
    "               'Excellent': 4,\n",
    "               '❗ Problematic Content ❗': 0, # for security\n",
    "            }\n",
    "\n",
    "N = len(df_mikkel)\n",
    "R = 5 #number of ratings\n",
    "\n",
    "score_dict = {}\n",
    "score_data = []\n",
    "\n",
    "for i in range(len(df_mikkel)):\n",
    "    labels = [df_carl[\"our_label\"].iloc[i], df_mikkel[\"our_label\"].iloc[i], df_david[\"our_label\"].iloc[i]] + df_mikkel[\"original_label\"].iloc[i]\n",
    "    scores = [num_encoder[label] for label in labels]\n",
    "    id_ = df_mikkel[\"id\"].iloc[i]\n",
    "    score_dict[id_] = scores #dict of lists\n",
    "    score_data.append(scores) # list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_score_data = [scores[:3] for scores in score_data]  # Only our labels (Mikkel, David, Carl)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, N + 1), [np.mean(scores) for scores in our_score_data], marker='o')\n",
    "plt.title('Mean scores of Authors over Samples')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea7196",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## Global aggreement: MSD and Krippendorffs $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b1da3",
   "metadata": {},
   "source": [
    "### MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Mean_Squared_Diff(score_data): # <-- list of lists\n",
    "\n",
    "    SD_list = []\n",
    "\n",
    "    for scores in score_data:\n",
    "        c = len(scores)\n",
    "        #The Squared Difference between unordered distinct pairs for one text:\n",
    "        SD = (2 / (c * (c - 1))) * sum(\n",
    "            (scores[j] - scores[k]) ** 2 \n",
    "            for j in range(c) \n",
    "            for k in range(j+1,c))\n",
    "        SD_list.append(SD)\n",
    "\n",
    "    return np.mean(SD_list)\n",
    "\n",
    "MSD = np.mean(Mean_Squared_Diff(score_data))\n",
    "\n",
    "MSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d573b5",
   "metadata": {},
   "source": [
    "### Krippendorff's $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha calculated with library\n",
    "def K_alpha(score_data,datatype):\n",
    "    reliability_data = np.array(score_data).T \n",
    "    alpha = krippendorff.alpha(reliability_data=reliability_data, value_domain=list(range(R)), level_of_measurement=datatype)\n",
    "    return alpha\n",
    "\n",
    "alpha = K_alpha(score_data,\"interval\")\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ad0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha calculated by hand assuming equal intervals:\n",
    "\n",
    "D_o = MSD\n",
    "\n",
    "# All annotation scores\n",
    "all_scores = np.asarray(score_data).flatten()\n",
    "\n",
    "\n",
    "label_counts = Counter(all_scores)  #dict\n",
    "labels = list(range(R))\n",
    "n_total = len(all_scores)  # total annotations\n",
    "\n",
    "# Calculate D_e\n",
    "De_numerator = 0\n",
    "for i in range(R):\n",
    "    for j in range(i + 1, R):\n",
    "        a = labels[i]\n",
    "        b = labels[j]\n",
    "        n_a = label_counts[a]\n",
    "        n_b = label_counts[b]\n",
    "        delta = (a - b) ** 2\n",
    "        De_numerator += n_a * n_b * delta\n",
    "\n",
    "D_e =  De_numerator * 2 / (n_total * (n_total - 1))\n",
    "\n",
    "alpha_simplifed = 1 - D_o / D_e\n",
    "alpha_simplifed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb98f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing to ordinal delta function\n",
    "alpha_ordinal = K_alpha(score_data, \"ordinal\")\n",
    "diff_alpha = alpha - alpha_ordinal\n",
    "\n",
    "print(f\"alpha (interval): {alpha}\")\n",
    "print(f\"alpha (ordinal): {alpha_ordinal}\")\n",
    "print(f\"Difference (alpha - alpha_ordinal): {diff_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ee51d",
   "metadata": {},
   "source": [
    "_______\n",
    "## Comparing annotator-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8b7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 #number of author annotators\n",
    "\n",
    "data = {\"author\": [scores[:n] for scores in score_data],\n",
    "        \"fineweb\": [scores[n:] for scores in score_data],\n",
    "        \"all\": score_data}\n",
    "\n",
    "agreement = {}\n",
    "for name,list_score in data.items():\n",
    "    agreement[name] = (Mean_Squared_Diff(list_score), \n",
    "                       K_alpha(list_score, \"ordinal\"),\n",
    "                       np.mean([np.mean(scores) for scores in list_score])\n",
    "                       )\n",
    "agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1af76",
   "metadata": {},
   "source": [
    "### Statistic tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6489dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality for MSD\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# first we define the MSD function:\n",
    "def Squared_diff_list(score_data): # <-- list of lists\n",
    "\n",
    "    SD_list = []\n",
    "\n",
    "    for scores in score_data:\n",
    "        c = len(scores)\n",
    "        #The Squared Difference between unordered distinct pairs for one text:\n",
    "        SD = (2 / (c * (c - 1))) * sum(\n",
    "            (scores[j] - scores[k]) ** 2 \n",
    "            for j in range(c) \n",
    "            for k in range(j+1,c))\n",
    "        SD_list.append(SD)\n",
    "\n",
    "    return SD_list\n",
    "\n",
    "# Shapiro Wilk\n",
    "stat, p_aut = shapiro(Squared_diff_list(data[\"author\"]))\n",
    "print(f\"Shapiro-Wilk p 'author' = {p_aut}\")\n",
    "\n",
    "stat, p_fine = shapiro(Squared_diff_list(data[\"fineweb\"]))\n",
    "print(f\"Shapiro-Wilk p 'fineweb' = {p_fine}\")\n",
    "\n",
    "np.mean(Squared_diff_list(score_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_dist(sample1, sample2, name1, name2, type):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    bins = [0.5 * x - 0.5 for x in range(16)]\n",
    "\n",
    "    if type == \"msd\":\n",
    "        list1 = Squared_diff_list(sample1)\n",
    "        list2 = Squared_diff_list(sample2)\n",
    "\n",
    "    elif type == \"mean\":\n",
    "        list1 = [np.mean(scores) for scores in sample1]\n",
    "        if sample2[0] is float:\n",
    "            list2 = sample2\n",
    "        else:\n",
    "            list2 = [np.mean(scores) for scores in sample2]\n",
    "        \n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(list1, bins=bins, edgecolor='black', color='skyblue', alpha=0.5, label=name1, rwidth=0.9)\n",
    "    plt.hist(list2, bins=bins, edgecolor='black', color='salmon', alpha=0.5, label=name2, rwidth=0.9)\n",
    "\n",
    "    # Customize axes\n",
    "    plt.xlabel(type.upper() + ' Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Clean layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c339bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dist(data[\"author\"],data[\"fineweb\"], \"Author\", \"FineWeb\", \"msd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dcfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dist(data[\"author\"],data[\"fineweb\"], \"Author\", \"FineWeb\", \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a49902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paired non-parametric test MSD\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def wilcoxon_test(sample1, sample2 , type):\n",
    "    if type == \"msd\":\n",
    "        list1 = Squared_diff_list(sample1)\n",
    "        list2 = Squared_diff_list(sample2)\n",
    "\n",
    "    elif type == \"mean\":\n",
    "        if sample1[0] is float:\n",
    "            list1 = sample1\n",
    "        else:\n",
    "            list1 = [np.mean(scores) for scores in sample1]\n",
    "        if sample2[0] is float:\n",
    "            list2 = sample2\n",
    "        else:\n",
    "            list2 = [np.mean(scores) for scores in sample2]\n",
    "    else:\n",
    "        \"Error\"\n",
    "    #print(len(list1),len(list2))\n",
    "    return wilcoxon(list1,list2) # returns --> wilcoxon_stat, wilcoxon_p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21503c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired test\n",
    "wilcoxon_stat_msd, wilcoxon_p_msd = wilcoxon_test(data[\"author\"], data[\"fineweb\"], \"msd\")\n",
    "wilcoxon_stat_mean, wilcoxon_p_mean = wilcoxon_test(data[\"author\"], data[\"fineweb\"], \"mean\")\n",
    "\n",
    "print(f\"Wilcoxon p-value (MSD): {wilcoxon_p_msd}\")\n",
    "print(f\"Wilcoxon p-value (Mean): {wilcoxon_p_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253c586",
   "metadata": {},
   "source": [
    "### Comparing our trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.read_json(\"../test/final_results.json\", lines=True)\n",
    "\n",
    "model_data = defaultdict(list)\n",
    "\n",
    "raw_cols = [col for col in df_models.columns if col.startswith(\"raw\")]\n",
    "for col in raw_cols:\n",
    "    model_data[col] = df_models[col].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff349c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals for model MSEs\n",
    "\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "y_human = np.mean([np.mean(scores) for scores in score_data])\n",
    "\n",
    "# Bootstrap parameters\n",
    "n_bootstrap = 10000\n",
    "alpha = 0.05\n",
    "\n",
    "model_names = []\n",
    "mean_errors = []\n",
    "cis_lower = []\n",
    "cis_upper = []\n",
    "\n",
    "for name, predictions in model_data.items():\n",
    "    preds = np.array(predictions)\n",
    "    errors = (y_human - preds) ** 2\n",
    "\n",
    "    model_names.append(name[15:].capitalize())\n",
    "\n",
    "    res = bootstrap((errors,), np.mean, confidence_level=1 - alpha, \n",
    "                    n_resamples=n_bootstrap, method='bca')\n",
    "\n",
    "    mean_errors.append(np.mean(errors))\n",
    "    cis_lower.append(res.confidence_interval.low)\n",
    "    cis_upper.append(res.confidence_interval.high)\n",
    "\n",
    "mean_errors = np.array(mean_errors)\n",
    "cis_lower = np.array(cis_lower)\n",
    "cis_upper = np.array(cis_upper)\n",
    "\n",
    "# Compute error bars\n",
    "error_lower = mean_errors - cis_lower\n",
    "error_upper = cis_upper - mean_errors\n",
    "yerr = [error_lower, error_upper]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = sns.color_palette(\"deep\", len(model_names))\n",
    "\n",
    "plt.errorbar(model_names, mean_errors, yerr=yerr,\n",
    "             fmt='o', markersize=8, capsize=6, capthick=2,\n",
    "             ecolor='gray', color='black', elinewidth=1.5)\n",
    "\n",
    "\n",
    "plt.axhline(min(mean_errors), color='red', linestyle='--', linewidth=1, label='Best model MSE', alpha=0.5)\n",
    "\n",
    "plt.title(\"Bootstrapped 95% Confidence Intervals for Model MSE\", fontsize=14)\n",
    "plt.ylabel(\"Mean Squared Error\", fontsize=12)\n",
    "plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f288d0d",
   "metadata": {},
   "source": [
    "#### Comparing to Human scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for normality of human scores\n",
    "shapiro_stat, shapiro_p = shapiro([np.mean(scores) for scores in score_data])\n",
    "float(shapiro_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf57692",
   "metadata": {},
   "source": [
    "No significance (p_shapiro < 0.05)  --> non-parametric tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc30bc",
   "metadata": {},
   "source": [
    "All models predicted significantly different scores than the human mean. (p-value > 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSD comparison with human scores\n",
    "\n",
    "for name, scores in model_data.items():\n",
    "    data_model_and_all = [[scores[i]] + data[\"all\"][i] for i in range(len(scores))] \n",
    "    wilcoxon_stat, wilcoxon_p = wilcoxon_test(score_data, data_model_and_all, \"msd\")\n",
    "    MSD_score = Mean_Squared_Diff(data_model_and_all)\n",
    "    print(f\"Model {name}, MSD: {MSD_score}, Wilcoxon p-value: {wilcoxon_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12960ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "# Plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Get the human baseline MSD\n",
    "y_human = np.asarray(Squared_diff_list(score_data))\n",
    "\n",
    "# Bootstrap parameters\n",
    "n_bootstrap = 10000\n",
    "alpha = 0.05\n",
    "\n",
    "model_names = []\n",
    "mean_errors = []\n",
    "cis_lower = []\n",
    "cis_upper = []\n",
    "\n",
    "# Add raw human data as a baseline comparison\n",
    "model_data_human = model_data.copy() \n",
    "model_data_human[\"Human\"] = score_data\n",
    "\n",
    "# Loop through models (and human baseline)\n",
    "for name, predictions in model_data_human.items():\n",
    "    # Format model name nicely\n",
    "    model_names.append(name[15:].capitalize() if name != \"Human\" else \"Human\")\n",
    "\n",
    "    if name != \"Human\":\n",
    "        # Add model predictions to human ratings per sample\n",
    "        combined = [np.append(score_data[i], predictions[i]) for i in range(len(predictions))]\n",
    "    else:\n",
    "        combined = score_data\n",
    "\n",
    "    # Compute per-sample MSD\n",
    "    y_all = np.asarray(Squared_diff_list(combined))\n",
    "\n",
    "    # Bootstrap CI of the MSD\n",
    "    res = bootstrap((y_all,), np.mean, confidence_level=1 - alpha, \n",
    "                    n_resamples=n_bootstrap, method='BCa')\n",
    "\n",
    "    mean_errors.append(np.mean(y_all))\n",
    "    cis_lower.append(res.confidence_interval.low)\n",
    "    cis_upper.append(res.confidence_interval.high)\n",
    "\n",
    "# Convert to arrays for plotting\n",
    "mean_errors = np.array(mean_errors)\n",
    "cis_lower = np.array(cis_lower)\n",
    "cis_upper = np.array(cis_upper)\n",
    "\n",
    "# Error bars\n",
    "error_lower = mean_errors - cis_lower\n",
    "error_upper = cis_upper - mean_errors\n",
    "yerr = [error_lower, error_upper]\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = sns.color_palette(\"deep\", len(model_names))\n",
    "\n",
    "plt.errorbar(model_names, mean_errors, yerr=yerr,\n",
    "             fmt='o', markersize=8, capsize=6, capthick=2,\n",
    "             ecolor='gray', color='black', elinewidth=1.5)\n",
    "\n",
    "# Optional: horizontal line for lowest MSD\n",
    "plt.axhline(min(mean_errors), color='blue', linestyle='--', linewidth=1, label='Lowest MSD', alpha=0.5)\n",
    "\n",
    "plt.title(\"Bootstrapped 95% Confidence Intervals for Model MSD\", fontsize=14)\n",
    "plt.ylabel(\"Mean Squared Difference (MSD)\", fontsize=12)\n",
    "plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717aff63",
   "metadata": {},
   "source": [
    "No models significantly raise the inter-annotator disagreement (MSD) when added the human annotators (p-values > 0.05). The MSD before the models where added was 1.26. However, more test data might reduce the margins ofthe CIs and show otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import krippendorff  # pip install krippendorff\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume annotations: shape (n_items, n_raters)\n",
    "# Example: annotations[i][j] = rating from rater j on item i\n",
    "def bootstrap_alpha(score_data):\n",
    "    annotations = np.array(score_data)  # shape: (N, 6)\n",
    "\n",
    "    n_bootstrap = 1000\n",
    "    alpha_values = []\n",
    "\n",
    "    for _ in tqdm(range(n_bootstrap)):\n",
    "        # Sample rows with replacement\n",
    "        indices = np.random.choice(len(annotations), size=len(annotations), replace=True)\n",
    "        resampled = annotations[indices]\n",
    "\n",
    "        # Transpose to shape (n_raters, n_items)\n",
    "        data = resampled.T\n",
    "\n",
    "        # Compute Krippendorff's alpha (interval scale assumed here)\n",
    "        alpha = krippendorff.alpha(reliability_data=data, level_of_measurement='interval')\n",
    "        alpha_values.append(alpha)\n",
    "\n",
    "    alpha_values = np.array(alpha_values)\n",
    "\n",
    "    # Compute 95% CI (percentile method)\n",
    "    lower = np.percentile(alpha_values, 2.5)\n",
    "    upper = np.percentile(alpha_values, 97.5)\n",
    "    mean_alpha = np.mean(alpha_values)\n",
    "\n",
    "    return [mean_alpha, lower, upper]\n",
    "\n",
    "CI_data = []\n",
    "model_names = []\n",
    "for name, predictions in model_data_human.items():\n",
    "    model_names.append(name[15:].capitalize() if name != \"Human\" else \"Human\")\n",
    "\n",
    "    if name != \"Human\":\n",
    "        # Add model predictions to human ratings per sample\n",
    "        combined = [np.append(score_data[i], predictions[i]) for i in range(len(predictions))]\n",
    "    else:\n",
    "        combined = score_data\n",
    "    \n",
    "    CI_data.append(bootstrap_alpha(combined))\n",
    "\n",
    "\n",
    "def plot_CI(data, labels=None, title=\"Bootstrapped 95% Confidence Intervals\", ylabel=\"Value\"):\n",
    "    \"\"\"\n",
    "    Plot mean and 95% CI for each entry in data.\n",
    "    data: list of [mean, lower, upper]\n",
    "    labels: list of str, optional\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    means = data[:, 0]\n",
    "    lowers = data[:, 1]\n",
    "    uppers = data[:, 2]\n",
    "    yerr = np.vstack([means - lowers, uppers - means])\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Item {i+1}\" for i in range(len(means))]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.errorbar(labels, means, yerr=yerr, fmt='o', markersize=8, capsize=6, capthick=2,\n",
    "                    ecolor='gray', color='black', elinewidth=1.5)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd17f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CI(CI_data, model_names, title=\"Bootstrapped 95% Confidence Intervals for Krippendorff's Alpha\", ylabel=\"Alpha Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7d914",
   "metadata": {},
   "source": [
    "___\n",
    "## Comparing classes\n",
    "We create pools for each annotation class, including all items where a specific class was annotated at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_pools = {}\n",
    "for i in range(R):\n",
    "    pool = [scores for scores in data[\"all\"] if i in scores]\n",
    "    agreement_pools[i] = (Mean_Squared_Diff(pool), K_alpha(pool,\"interval\"), K_alpha(pool,\"nominal\"))\n",
    "                        \n",
    "    print(pool)\n",
    "\n",
    "agreement_pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "bins = [x - 0.5 for x in range(6)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(all_scores, bins=bins, edgecolor='black', color='skyblue', rwidth=0.9,)\n",
    "\n",
    "# Customize axes\n",
    "plt.xticks(range(5)) \n",
    "plt.xlabel('Annotation Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Clean layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean_Ordinal_Diff(scores): # <— one item\n",
    "    freqs = Counter(scores)\n",
    "    categories = sorted(freqs.keys())\n",
    "    total_pairs = 0\n",
    "    total_OD = 0\n",
    "\n",
    "    # Iterate over all unordered distinct pairs (a, b) where a < b\n",
    "    for i in range(len(categories)):\n",
    "        for j in range(i + 1, len(categories)):\n",
    "            a, b = categories[i], categories[j]\n",
    "\n",
    "            # Frequencies of a, b\n",
    "            n_a = freqs[a]\n",
    "            n_b = freqs[b]\n",
    "\n",
    "            # Sum of frequencies between a and b (inclusive)\n",
    "            n_sum = sum(freqs[g] for g in range(a, b + 1))\n",
    "\n",
    "            # Ordinal difference formula\n",
    "            delta_sq = (n_sum - (n_a + n_b) / 2) ** 2\n",
    "\n",
    "            # Count how many such pairs exist\n",
    "            pair_count = n_a * n_b\n",
    "            total_OD += delta_sq * pair_count\n",
    "            total_pairs += pair_count\n",
    "\n",
    "    # Normalize by total number of unordered pairs\n",
    "    if total_pairs > 0:\n",
    "        return total_OD / total_pairs\n",
    "    else:\n",
    "        return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FP25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
